{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BQ_INPUT_DATASET_NAME:  geospatial_wkt_v01 \n",
      "BQ_INPUT_TABLE_NAME:  y2018m11d14_rh_hybasgadm_union_whymap_bq_v01_v02 \n",
      "BQ_OUTPUT_DATASET_NAME: geospatial_wkt_v01 \n",
      "BQ_OUTPUT_TABLE_NAME:  y2018m11d28_rh_hybasgadmwhymap_to_gcs_v01_v01 \n",
      "GCS_OUTPUT_PATH:  gs://aqueduct30_v01/Y2018M11D28_RH_hybasgadmwhymap_To_GCS_V01/output_V01\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Export table with geographies as WKT to CSV file on GCS. \n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "Author: Rutger Hofste\n",
    "Date: 20181128\n",
    "Kernel: python35\n",
    "Docker: rutgerhofste/gisdocker:ubuntu16.04\n",
    "\n",
    "Args:\n",
    "    TESTING (Boolean) : Toggle testing case.\n",
    "    SCRIPT_\n",
    "    NAME (string) : Script name.\n",
    "    OUTPUT_VERSION (integer) : output version.\n",
    "    DATABASE_ENDPOINT (string) : RDS or postGreSQL endpoint.\n",
    "    DATABASE_NAME (string) : Database name.\n",
    "    \n",
    "    TABLE_NAME_AREA_30SPFAF06 (string) : Table name used for areas. Must exist\n",
    "        on same database as used in rest of script.\n",
    "    S3_INPUT_PATH_RIVERDISCHARGE (string) : AWS S3 input path for \n",
    "        riverdischarge.    \n",
    "    S3_INPUT_PATH_DEMAND (string) : AWS S3 input path for \n",
    "        demand.     \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "TESTING = 1\n",
    "SCRIPT_NAME = \"Y2018M11D28_RH_hybasgadmwhymap_To_GCS_V01\"\n",
    "OUTPUT_VERSION = 1\n",
    "\n",
    "\n",
    "BQ_PROJECT_ID = \"aqueduct30\"\n",
    "BQ_INPUT_DATASET_NAME = \"geospatial_geog_v01\"\n",
    "BQ_OUTPUT_DATASET_NAME = \"geospatial_wkt_v01\"\n",
    "BQ_INPUT_TABLE_NAME = \"y2018m11d14_rh_hybasgadm_union_whymap_bq_v01_v02\"\n",
    "BQ_OUTPUT_TABLE_NAME = \"{}_v{:02.0f}\".format(SCRIPT_NAME,OUTPUT_VERSION).lower()\n",
    "\n",
    "GCS_OUTPUT_PATH = \"gs://aqueduct30_v01/{}/output_V{:02.0f}\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "\n",
    "\n",
    "print(\"\\nBQ_INPUT_DATASET_NAME: \", BQ_OUTPUT_DATASET_NAME,\n",
    "      \"\\nBQ_INPUT_TABLE_NAME: \", BQ_INPUT_TABLE_NAME, \n",
    "      \"\\nBQ_OUTPUT_DATASET_NAME:\", BQ_OUTPUT_DATASET_NAME,\n",
    "      \"\\nBQ_OUTPUT_TABLE_NAME: \",BQ_OUTPUT_TABLE_NAME,\n",
    "      \"\\nGCS_OUTPUT_PATH: \", GCS_OUTPUT_PATH\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y2018M11D28 UTC 11:33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.4 |Anaconda, Inc.| (default, Nov 20 2017, 18:44:38) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, datetime, sys\n",
    "dateString = time.strftime(\"Y%YM%mD%d\")\n",
    "timeString = time.strftime(\"UTC %H:%M\")\n",
    "start = datetime.datetime.now()\n",
    "print(dateString,timeString)\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/.google.json\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"aqueduct30\"\n",
    "client = bigquery.Client(project=BQ_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "destination_dataset_ref = client.dataset(BQ_OUTPUT_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "destination_table_ref = destination_dataset_ref.table(BQ_OUTPUT_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_config.destination = destination_table_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT\n",
    "  id_pfafgadmwhymap,\n",
    "  ST_ASTEXT(g) as wkt\n",
    "FROM\n",
    "  `aqueduct30.{}.{}`\n",
    "\"\"\".format(BQ_INPUT_DATASET_NAME,BQ_INPUT_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query=q,\n",
    "                         job_config=job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exports geographies as WKT to new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export to GCS (https://cloud.google.com/bigquery/docs/exporting-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "destination_uri = \"{}/output_*.csv\".format(GCS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_job = client.extract_table(\n",
    "    destination_table_ref,\n",
    "    destination_uri,\n",
    "    # Location must match that of the source table.\n",
    "    location='US')  # API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7f41e85b8d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:50.233074\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Runs:  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 35",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
