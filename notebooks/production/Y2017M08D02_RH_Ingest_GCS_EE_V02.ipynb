{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input gcs: gs://aqueduct30_v01/Y2017M07D31_RH_Convert_NetCDF_Geotiff_V02/output_V01/\n",
      "Output ee: projects/WRI-Aquaduct/PCRGlobWB20V09\n",
      "Output S3: s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Ingest_GCS_EE_V02/output_V09\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Ingest PCRGLOBWB timeseries data on Google Earth Engine\n",
    "-------------------------------------------------------------------------------\n",
    "This notebook will upload the geotiff files from the Google Cloud Storage to\n",
    "the WRI/aqueduct earthengine bucket. An errorlog will be stored on Amazon S3.\n",
    "\n",
    "Requirements:\n",
    "    Authorize earthengine by running in your terminal: earthengine \n",
    "                                                       authenticate\n",
    "\n",
    "    you need to have access to the WRI-Aquaduct (yep a Google employee made a\n",
    "    typo) bucket to ingest the data. Rutger can grant access to write to this \n",
    "    folder. \n",
    "\n",
    "    Have access to the Google Cloud Storage Bucker\n",
    "\n",
    "Make sure to set the project to Aqueduct30 by running\n",
    "`gcloud config set project aqueduct30`\n",
    "\n",
    "Code follows the Google for Python Styleguide. Exception are the scripts that \n",
    "use earth engine since this is camelCase instead of underscore.\n",
    "\n",
    "Author: Rutger Hofste\n",
    "Date: 20170802\n",
    "Kernel: python27\n",
    "Docker: rutgerhofste/gisdocker:ubuntu16.04\n",
    "\n",
    "Args:    \n",
    "    TESTING (Boolean) : Toggle Testing Mode.\n",
    "    OVERWRITE (Boolean) : Overwrite old folder !CAUTION!\n",
    "    SCRIPT_NAME (string) : Script name.\n",
    "    \n",
    "    PREVIOUS_SCRIPT_NAME (string) : Previous script name. \n",
    "    INPUT_VERSION (integer) : Input version. \n",
    "    \n",
    "    OUTPUT_VERSION (integer) : Output version. \n",
    "    \n",
    "    OUTPUT_FILE_NAME (string) : File Name for a csv file containing the failed tasks. \n",
    "\n",
    "Returns:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Input Parameters\n",
    "TESTING = 1\n",
    "OVERWRITE = 0 # !CAUTION!\n",
    "SCRIPT_NAME = \"Y2017M08D02_RH_Ingest_GCS_EE_V02\"\n",
    "PREVIOUS_SCRIPT_NAME = \"Y2017M07D31_RH_Convert_NetCDF_Geotiff_V02\"\n",
    "\n",
    "INPUT_VERSION = 1\n",
    "OUTPUT_VERSION = 9\n",
    "\n",
    "OUTPUT_FILE_NAME = \"df_errorsV01.csv\"\n",
    "\n",
    "SEPARATOR = \"_|-\"\n",
    "SCHEMA = [\"geographic_range\",\n",
    "     \"temporal_range\",\n",
    "     \"indicator\",\n",
    "     \"temporal_resolution\",\n",
    "     \"unit\",\n",
    "     \"spatial_resolution\",\n",
    "     \"temporal_range_min\",\n",
    "     \"temporal_range_max\"]\n",
    "\n",
    "extra_properties = {\"nodata_value\":-9999,\n",
    "                    \"ingested_by\" : \"RutgerHofste\",\n",
    "                    \"script_used\": SCRIPT_NAME}\n",
    "\n",
    "# ETL\n",
    "gcs_input_path = \"gs://aqueduct30_v01/{}/output_V{:02.0f}/\".format(PREVIOUS_SCRIPT_NAME,INPUT_VERSION)\n",
    "ee_output_path = \"projects/WRI-Aquaduct/PCRGlobWB20V{:02.0f}\".format(OUTPUT_VERSION)\n",
    "s3_output_path = \"s3://wri-projects/Aqueduct30/processData/{}/output_V{:02.0f}\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "ec2_output_path = \"/volumes/data/{}/output_V{:02.0f}\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "\n",
    "print(\"Input gcs: \" +  gcs_input_path +\n",
    "      \"\\nOutput ee: \" + ee_output_path +\n",
    "      \"\\nOutput S3: \" + s3_output_path )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y2018M04D16 UTC 14:22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.4 |Anaconda, Inc.| (default, Nov 20 2017, 18:44:38) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, datetime, sys\n",
    "dateString = time.strftime(\"Y%YM%mD%d\")\n",
    "timeString = time.strftime(\"UTC %H:%M\")\n",
    "start = datetime.datetime.now()\n",
    "print(dateString,timeString)\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import subprocess\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import aqueduct3\n",
    "\n",
    "\n",
    "# ETL\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    if OVERWRITE:\n",
    "        command = \"earthengine rm -r {}\".format(ee_output_path)\n",
    "        print(command)\n",
    "        subprocess.check_output(command,shell=True)\n",
    "\n",
    "    command = \"earthengine create folder {}\".format(ee_output_path)\n",
    "    print(command)\n",
    "    subprocess.check_output(command,shell=True)\n",
    "\n",
    "    # Script\n",
    "\n",
    "    \n",
    "    keys = aqueduct3.get_GCS_keys(gcs_input_path)\n",
    "    df = aqueduct3.keys_to_df(keys,separator,schema)\n",
    "\n",
    "    df = df.assign(**extra_properties)\n",
    "    df[\"exportdescription\"] = df[\"indicator\"] + \"_\" + df[\"temporal_resolution\"]+\"Y\"+df[\"year\"]+\"M\"+df[\"month\"]\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # Create ImageCollections\n",
    "    parameters = df.parameter.unique()\n",
    "    for parameter in parameters:\n",
    "        ic_id = ee_output_path + \"/\" + parameter\n",
    "        command, result = aqueduct3.create_imageCollection(ic_id)\n",
    "        print(command,result)\n",
    "\n",
    "    if TESTING:\n",
    "        df_complete = df_complete[1:3]\n",
    "        \n",
    "        \n",
    "    df_errors = pd.DataFrame()\n",
    "    \n",
    "    for index, row in df_complete.iterrows():\n",
    "        elapsed_time = time.time() - start_time \n",
    "        print(index,\"%.2f\" %((index/df_complete.shape[0])*100), \"elapsed: \", str(timedelta(seconds=elapsed_time)))\n",
    "        geotiff_gcs_path = GCS_BASE + row.file_name + \".\" + row.extension\n",
    "        output_ee_asset_id = EE_BASE +\"/\"+ row.parameter + \"/\" + row.file_name\n",
    "        properties = row.to_dict()\n",
    "\n",
    "        df_errors2 = aqueduct3.upload_geotiff_to_EE_imageCollection(geotiff_gcs_path, output_ee_asset_id, properties,index)\n",
    "        df_errors = df_errors.append(df_errors2)    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding NoData value, ingested_by and exportdescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p {ec2_output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_errors.to_csv(\"{}/{}\".format(ec2_output_path,OUTPUT_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp  {ec2_output_path} {S3_OUTPUT_PATH} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retry the ones with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_retry = df_errors.loc[df_errors['error'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_retry.iterrows():\n",
    "    response = subprocess.check_output(row.command, shell=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniques = df_errors[\"error\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def split_key(key):\n",
    "    \"\"\" Split key into dictionary\n",
    "    -------------------------------------------------------------------------------\n",
    "    WARNING: This function is dependant on the name convention of PCRGLOBWB\n",
    "    Do not use with other keys\n",
    "    \n",
    "    Args:\n",
    "        key (string) : key containing information about parameter, year month etc.\n",
    "        \n",
    "    Returns:\n",
    "        out_dict (dictionary): Dictionary containing all information contained\n",
    "                               in key.      \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # will yield the root file code and extension of a set of keys\n",
    "    prefix, extension = key.split(\".\")\n",
    "    file_name = prefix.split(\"/\")[-1]\n",
    "    parameter = file_name[:-12]\n",
    "    month = file_name[-2:] #can also do this with regular expressions if you like\n",
    "    year = file_name[-7:-3]\n",
    "    identifier = file_name[-11:-8]\n",
    "    out_dict = {\"file_name\":file_name,\"extension\":extension,\"parameter\":parameter,\"month\":month,\"year\":year,\"identifier\":identifier}\n",
    "    return out_dict\n",
    "\n",
    "def split_parameter(parameter):\n",
    "    \"\"\"Split parameter \n",
    "    -------------------------------------------------------------------------------\n",
    "    WARNING: This function is dependant on the name convention of PCRGLOBWB\n",
    "    Do not use with other keys.\n",
    "    \n",
    "    Args:\n",
    "        parameter (string) : parameter string.\n",
    "    \n",
    "    Returns:\n",
    "        out_dict (dictionary) : dictionary containing all information contained\n",
    "                                in parameter key.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    values = re.split(\"_|-\", parameter) #soilmoisture uses a hyphen instead of underscore between the years\n",
    "    keys = [\"geographic_range\",\"temporal_range\",\"indicator\",\"temporal_resolution\",\"units\",\"spatial_resolution\",\"temporal_range_min\",\"temporal_range_max\"]\n",
    "    # ['global', 'historical', 'PDomWN', 'month', 'millionm3', '5min', '1960', '2014']\n",
    "    out_dict = dict(zip(keys, values))\n",
    "    out_dict[\"parameter\"] = parameter\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def get_GCS_keys(GCS_BASE):\n",
    "    \"\"\" get list of keys from Google Cloud Storage\n",
    "    -------------------------------------------------------------------------------\n",
    "    \n",
    "    Args:\n",
    "        GCS_BASE (string) : Google Cloud Storage namespace containing files.\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame) : DataFrame with properties useful to Aqueduct. \n",
    "    \n",
    "    \"\"\"\n",
    "    command = \"/opt/google-cloud-sdk/bin/gsutil ls {}\".format(GCS_BASE)\n",
    "    keys = subprocess.check_output(command,shell=True)\n",
    "    keys = keys.decode('UTF-8').splitlines()\n",
    "    \n",
    "    df = keys_to_df(keys)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def keys_to_df(keys):\n",
    "    \"\"\" helper function for 'get_GCS_keys'\n",
    "    \n",
    "    Args:\n",
    "        keys (list) : list of strings with keys.\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame) : Pandas DataFrame with all relvant properties for\n",
    "                            Aqueduct 3.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    i = 0\n",
    "    for key in keys:\n",
    "        i = i+1\n",
    "        out_dict = split_key(key)\n",
    "        df2 = pd.DataFrame(out_dict,index=[i])\n",
    "        df = df.append(df2)    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 35",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
