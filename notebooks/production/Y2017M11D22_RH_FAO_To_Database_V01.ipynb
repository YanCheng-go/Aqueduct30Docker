{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store FAO related files on PostgreSQL RDS database\n",
    "\n",
    "* Purpose of script: This script will process the hydrobasin related data into multiple tables according to the database ERD\n",
    "* Author: Rutger Hofste\n",
    "* Kernel used: python35\n",
    "* Date created: 20171122 \n",
    "\n",
    "The script requires a file called .password to be stored in the current working directory with the password to the database. Basic functionality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y2019M01D04 UTC 14:03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.4 |Anaconda, Inc.| (default, Nov 20 2017, 18:44:38) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time, datetime, sys\n",
    "dateString = time.strftime(\"Y%YM%mD%d\")\n",
    "timeString = time.strftime(\"UTC %H:%M\")\n",
    "start = datetime.datetime.now()\n",
    "print(dateString,timeString)\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCRIPT_NAME = \"Y2017M11D22_RH_FAO_To_Database_V01\"\n",
    "\n",
    "INPUT_VERSION = 1\n",
    "INPUT_VERSION_LINK = 4\n",
    "OUTPUT_VERSION = 7\n",
    "\n",
    "INPUT_FILE_NAME = \"hydrobasins_fao_fiona_merged_v%0.2d\" %(INPUT_VERSION)\n",
    "INPUT_FILE_NAME_LINK = \"hybas_lev06_v1c_merged_fiona_withFAO_V%0.2d_link\" %(INPUT_VERSION_LINK)\n",
    "\n",
    "EC2_INPUT_PATH = \"/volumes/data/%s/input/\" %(SCRIPT_NAME)\n",
    "EC2_OUTPUT_PATH = \"/volumes/data/%s/output/\" %(SCRIPT_NAME)\n",
    "\n",
    "S3_INPUT_PATH = \"s3://wri-projects/Aqueduct30/processData/Y2017M08D23_RH_Merge_FAONames_V01/output/\"\n",
    "S3_INPUT_PATH_LINK = \"s3://wri-projects/Aqueduct30/processData/Y2017M08D25_RH_spatial_join_FAONames_V01/output/\"\n",
    "\n",
    "# Database settings\n",
    "DATABASE_IDENTIFIER = \"aqueduct30v02\"\n",
    "DATABASE_NAME = \"database01\"\n",
    "\n",
    "TABLE_NAME_FAO_MAJOR = \"fao_major_v%0.2d\" %(OUTPUT_VERSION)\n",
    "TABLE_NAME_FAO_MINOR = \"fao_minor_v%0.2d\" %(OUTPUT_VERSION)\n",
    "TABLE_NAME_FAO_MINOR_TEMP = \"fao_minor_temp_v%0.2d\" %(OUTPUT_VERSION)\n",
    "TABLE_NAME_FAO_LINK = \"fao_link_v%0.2d\" %(OUTPUT_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/volumes/data/Y2017M11D22_RH_FAO_To_Database_V01/input/': No such file or directory\n",
      "rm: cannot remove '/volumes/data/Y2017M11D22_RH_FAO_To_Database_V01/output/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r {EC2_INPUT_PATH}\n",
    "!rm -r {EC2_OUTPUT_PATH}\n",
    "\n",
    "!mkdir -p {EC2_INPUT_PATH}\n",
    "!mkdir -p {EC2_OUTPUT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D23_RH_Merge_FAONames_V01/output/hydrobasins_fao_fiona_merged_v01.cpg to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hydrobasins_fao_fiona_merged_v01.cpg\n",
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D23_RH_Merge_FAONames_V01/output/hydrobasins_fao_fiona_merged_v01.prj to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hydrobasins_fao_fiona_merged_v01.prj\n",
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D23_RH_Merge_FAONames_V01/output/hydrobasins_fao_fiona_merged_v01.shx to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hydrobasins_fao_fiona_merged_v01.shx\n",
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D23_RH_Merge_FAONames_V01/output/hydrobasins_fao_fiona_merged_v01.dbf to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hydrobasins_fao_fiona_merged_v01.dbf\n",
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D23_RH_Merge_FAONames_V01/output/hydrobasins_fao_fiona_merged_v01.shp to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hydrobasins_fao_fiona_merged_v01.shp\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {S3_INPUT_PATH} {EC2_INPUT_PATH} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D25_RH_spatial_join_FAONames_V01/output/hybas_lev06_v1c_merged_fiona_withFAO_V04.pkl to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hybas_lev06_v1c_merged_fiona_withFAO_V04.pkl\n",
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D25_RH_spatial_join_FAONames_V01/output/hybas_lev06_v1c_merged_fiona_withFAO_V04.csv to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hybas_lev06_v1c_merged_fiona_withFAO_V04.csv\n",
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D25_RH_spatial_join_FAONames_V01/output/hybas_lev06_v1c_merged_fiona_withFAO_V04_link.pkl to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hybas_lev06_v1c_merged_fiona_withFAO_V04_link.pkl\n",
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D25_RH_spatial_join_FAONames_V01/output/hybas_lev06_v1c_merged_fiona_withFAO_V04_link.csv to ../../../../data/Y2017M11D22_RH_FAO_To_Database_V01/input/hybas_lev06_v1c_merged_fiona_withFAO_V04_link.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {S3_INPUT_PATH_LINK} {EC2_INPUT_PATH} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from ast import literal_eval\n",
    "import boto3\n",
    "import botocore\n",
    "from sqlalchemy import *\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "from shapely.geometry.multipolygon import MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RDS Connection\n",
    "def rdsConnect(database_identifier,database_name):\n",
    "    rds = boto3.client('rds')\n",
    "    F = open(\".password\",\"r\")\n",
    "    password = F.read().splitlines()[0]\n",
    "    F.close()\n",
    "    response = rds.describe_db_instances(DBInstanceIdentifier=\"%s\"%(database_identifier))\n",
    "    status = response[\"DBInstances\"][0][\"DBInstanceStatus\"]\n",
    "    print(\"Status:\",status)\n",
    "    endpoint = response[\"DBInstances\"][0][\"Endpoint\"][\"Address\"]\n",
    "    print(\"Endpoint:\",endpoint)\n",
    "    engine = create_engine('postgresql://rutgerhofste:%s@%s:5432/%s' %(password,endpoint,database_name))\n",
    "    connection = engine.connect()\n",
    "    return engine, connection\n",
    "\n",
    "def uploadGDFtoPostGIS(gdf,tableName,saveIndex):\n",
    "    # this function uploads a polygon shapefile to table in AWS RDS. \n",
    "    # It handles combined polygon/multipolygon geometry and stores it in valid multipolygon in epsg 4326.\n",
    "    \n",
    "    # gdf = input geoDataframe\n",
    "    # tableName = postGIS table name (string)\n",
    "    # saveIndex = save index column in separate column in postgresql, otherwise discarded. (Boolean)\n",
    "    \n",
    "    \n",
    "    gdf[\"type\"] = gdf.geometry.geom_type    \n",
    "    geomTypes = [\"Polygon\",\"MultiPolygon\"]\n",
    "    \n",
    "    for geomType in geomTypes:\n",
    "        gdfType = gdf.loc[gdf[\"type\"]== geomType]\n",
    "        geomTypeLower = str.lower(geomType)\n",
    "        gdfType['geom'] = gdfType['geometry'].apply(lambda x: WKTElement(x.wkt, srid=4326))\n",
    "        gdfType.drop([\"geometry\",\"type\"],1, inplace=True)      \n",
    "        print(\"Create table temp%s\" %(geomTypeLower)) \n",
    "        gdfType.to_sql(\n",
    "            name = \"temp%s\" %(geomTypeLower),\n",
    "            con = engine,\n",
    "            if_exists='replace',\n",
    "            index= saveIndex, \n",
    "            dtype={'geom': Geometry(str.upper(geomType), srid= 4326)}\n",
    "        )\n",
    "        \n",
    "    # Merge both tables and make valid\n",
    "    sql = []\n",
    "    sql.append(\"DROP TABLE IF EXISTS %s\"  %(tableName))\n",
    "    sql.append(\"ALTER TABLE temppolygon ALTER COLUMN geom type geometry(MultiPolygon, 4326) using ST_Multi(geom);\")\n",
    "    sql.append(\"CREATE TABLE %s AS (SELECT * FROM temppolygon UNION SELECT * FROM tempmultipolygon);\" %(tableName))\n",
    "    sql.append(\"UPDATE %s SET geom = st_makevalid(geom);\" %(tableName))\n",
    "    sql.append(\"DROP TABLE temppolygon,tempmultipolygon\")\n",
    "\n",
    "    for statement in sql:\n",
    "        print(statement)\n",
    "        result = connection.execute(statement)    \n",
    "    gdfFromSQL =gpd.GeoDataFrame.from_postgis(\"select * from %s\" %(tableName),connection,geom_col='geom' )\n",
    "    return gdfFromSQL\n",
    "\n",
    "\n",
    "def postGISDissolveFirst(sourceTableName,targetTableName,by):\n",
    "    #dissolve polygons and polygon related attributes (area)\n",
    "    #take first attribute of other\n",
    "    \n",
    "    sql =   (\"CREATE TABLE %s AS SELECT MIN(%s) as %s,MIN(sub_bas) as sub_bas, \" \n",
    "            \"MIN(to_bas) as to_bas, \"\n",
    "            \"MIN(maj_bas) as maj_bas, \"\n",
    "            \"MIN(sub_name) as sub_name, \"\n",
    "            \"MIN(sub_area) as sub_area, \"\n",
    "            \"ST_Multi(ST_Union(t.geom)) as geom \"\n",
    "            \"FROM %s As t GROUP BY %s\") %(targetTableName,by, by, sourceTableName,by)\n",
    "    connection.execute(sql)\n",
    "    gdfFromSQL =gpd.GeoDataFrame.from_postgis(\"select * from %s\" %(targetTableName),connection,geom_col='geom' )\n",
    "    print(sql)\n",
    "    return gdfFromSQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoRegionError",
     "evalue": "You must specify a region.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoRegionError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d342e6f079a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdsConnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATABASE_IDENTIFIER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDATABASE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ed8e220c4576>\u001b[0m in \u001b[0;36mrdsConnect\u001b[0;34m(database_identifier, database_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# RDS Connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrdsConnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_identifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatabase_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".password\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/boto3/__init__.py\u001b[0m in \u001b[0;36mclient\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/boto3/session.py\u001b[0m in \u001b[0;36mclient\u001b[0;34m(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0maws_access_key_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maws_access_key_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0maws_secret_access_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maws_secret_access_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             aws_session_token=aws_session_token, config=config)\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     def resource(self, service_name, region_name=None, api_version=None,\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/botocore/session.py\u001b[0m in \u001b[0;36mcreate_client\u001b[0;34m(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0mis_secure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoped_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scoped_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             client_config=config, api_version=api_version)\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/botocore/client.py\u001b[0m in \u001b[0;36mcreate_client\u001b[0;34m(self, service_name, region_name, is_secure, endpoint_url, verify, credentials, scoped_config, api_version, client_config)\u001b[0m\n\u001b[1;32m     74\u001b[0m         client_args = self._get_client_args(\n\u001b[1;32m     75\u001b[0m             \u001b[0mservice_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_secure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             verify, credentials, scoped_config, client_config, endpoint_bridge)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mservice_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclient_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_retries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_get_client_args\u001b[0;34m(self, service_model, region_name, is_secure, endpoint_url, verify, credentials, scoped_config, client_config, endpoint_bridge)\u001b[0m\n\u001b[1;32m    289\u001b[0m         return args_creator.get_client_args(\n\u001b[1;32m    290\u001b[0m             \u001b[0mservice_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_secure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             verify, credentials, scoped_config, client_config, endpoint_bridge)\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/botocore/args.py\u001b[0m in \u001b[0;36mget_client_args\u001b[0;34m(self, service_model, region_name, is_secure, endpoint_url, verify, credentials, scoped_config, client_config, endpoint_bridge)\u001b[0m\n\u001b[1;32m     44\u001b[0m         final_args = self.compute_client_args(\n\u001b[1;32m     45\u001b[0m             \u001b[0mservice_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_bridge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             endpoint_url, is_secure, scoped_config)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mservice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'service_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/botocore/args.py\u001b[0m in \u001b[0;36mcompute_client_args\u001b[0;34m(self, service_model, client_config, endpoint_bridge, region_name, endpoint_url, is_secure, scoped_config)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         endpoint_config = endpoint_bridge.resolve(\n\u001b[0;32m--> 116\u001b[0;31m             service_name, region_name, endpoint_url, is_secure)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Override the user agent if specified in the client config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/botocore/client.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, service_name, region_name, endpoint_url, is_secure)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mregion_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_default_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         resolved = self.endpoint_resolver.construct_endpoint(\n\u001b[0;32m--> 364\u001b[0;31m             service_name, region_name)\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             return self._create_endpoint(\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/botocore/regions.py\u001b[0m in \u001b[0;36mconstruct_endpoint\u001b[0;34m(self, service_name, region_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpartition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'partitions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             result = self._endpoint_for_partition(\n\u001b[0;32m--> 122\u001b[0;31m                 partition, service_name, region_name)\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python35/lib/python3.5/site-packages/botocore/regions.py\u001b[0m in \u001b[0;36m_endpoint_for_partition\u001b[0;34m(self, partition, service_name, region_name)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mregion_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'partitionEndpoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNoRegionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Attempt to resolve the exact region for this partition.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregion_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mservice_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'endpoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoRegionError\u001b[0m: You must specify a region."
     ]
    }
   ],
   "source": [
    "engine, connection = rdsConnect(DATABASE_IDENTIFIER,DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join(EC2_INPUT_PATH,INPUT_FILE_NAME+\".shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf.columns = map(str.lower, gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to store the data in two tables: major basin and minor basin together with the geometry. There is no unique identifier for the minor basins so we will use a composite key    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compositeKey(maj_bas,sub_bas):\n",
    "    key = 'MAJ_BAS_%0.4d_SUB_BAS_%0.7d' %(maj_bas,sub_bas)\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf[\"fao_id\"]= gdf.apply(lambda x: compositeKey(x[\"maj_bas\"],x[\"sub_bas\"]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf = gdf.set_index(\"fao_id\",drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = gdf.drop(\"geometry\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfGrouped = df.groupby(df[\"fao_id\"]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGrouped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major River Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfMajorFull = gdf[[\"maj_bas\",\"maj_name\",\"maj_area\",\"legend\"]]\n",
    "gdfMinor = gdf[[\"sub_bas\",\"to_bas\",\"maj_bas\",\"sub_name\",\"sub_area\",\"geometry\",\"fao_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMajorFull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfMajor = dfMajorFull.groupby(\"maj_bas\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMajor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfMajor.to_sql(\n",
    "    name = TABLE_NAME_FAO_MAJOR,\n",
    "    con = connection,\n",
    "    if_exists=\"replace\",\n",
    "    index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor River Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfMinor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometry consists of polygon and multipolygon type. Upload both to postGIS and set polygon to multipolygon and join. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfFromSQL = uploadGDFtoPostGIS(gdfMinor,TABLE_NAME_FAO_MINOR_TEMP,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfFromSQL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = gdfFromSQL.duplicated(subset=\"fao_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfFromSQL2 = postGISDissolveFirst(TABLE_NAME_FAO_MINOR_TEMP,TABLE_NAME_FAO_MINOR,\"fao_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"DROP TABLE IF EXISTS %s\" %(TABLE_NAME_FAO_MINOR_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dimensions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfFromSQL2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGrouped.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Table\n",
    "\n",
    "this information comes from a spatial join and is stored in a different table. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_link = pd.read_pickle(os.path.join(EC2_INPUT_PATH,INPUT_FILE_NAME_LINK+\".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_link = df_link.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_link.drop(\"index\",1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_link.index.names = ['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_link.columns = map(str.lower, df_link.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_link.columns = [\"pfaf_id\",\"fao_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_link.to_sql(\n",
    "    name = TABLE_NAME_FAO_LINK,\n",
    "    con = connection,\n",
    "    if_exists=\"replace\",\n",
    "    index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 35",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
