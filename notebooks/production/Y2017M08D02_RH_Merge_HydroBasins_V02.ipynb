{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input s3: s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/\n",
      "Input ec2: /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/input_V01/\n",
      "Output ec2: /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/\n",
      "Output S3: s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/\n",
      "Output gcs: gs://aqueduct30_v01/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/\n",
      "Output ee: projects/WRI-Aquaduct/PCRGlobWB20V09\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Merge WWF's HydroBasins\n",
    "-------------------------------------------------------------------------------\n",
    "Copy the relevant files from S3 raw to S3 process.\n",
    "Merge the shapefiles of level 6 and level 0 using Fiona.\n",
    "Rasterize shapefiles using gdal_rasterize (CLI)\n",
    "Uploads to S3 and GCS.\n",
    "\n",
    "Author: Rutger Hofste\n",
    "Date: 20170802\n",
    "Kernel: python35\n",
    "Docker: rutgerhofste/gisdocker:ubuntu16.04\n",
    "\n",
    "Args:\n",
    "    SCRIPT_NAME (string) : Script name\n",
    "    S3_INPUT_PATH (string) : Name of script used as input. \n",
    "    INPUT_VERSION (integer) : Input version.\n",
    "    OUTPUT_VERSION (integer) : Output version.\n",
    "    S3_INPUT_PATH (string) : S3 input path. Hardcoded since most upstream.\n",
    "    GDAL_RASTERIZE_PATH (string) : GDAL version used.\n",
    "    X_DIMENSION_5MIN (integer) : horizontal or longitudinal dimension of \n",
    "        raster at 5 arcminutes resolution.\n",
    "    Y_DIMENSION_5MIN (integer) : vertical or latitudinal dimension of \n",
    "        raster at 5 arcminutes resolution.\n",
    "    X_DIMENSION_30S (integer) : horizontal or longitudinal dimension of \n",
    "        raster at 30 arcseconds resolution.\n",
    "    Y_DIMENSION_30S (integer) : vertical or latitudinal dimension of \n",
    "        raster at 30 arcseconds resolution.\n",
    "    SPATIAL_RESOLUTIONS (list) : Spatial Resolutions used for rasterization.\n",
    "        Supported are '5min' and '30s'. List of strings.\n",
    "    PFAF_LEVELS (list) : Pfafstetter code used for rasterization.\n",
    "        Supported are '06' and '00'. List of Strings.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Input Parameters\n",
    "\n",
    "SCRIPT_NAME = \"Y2017M08D02_RH_Merge_HydroBasins_V02\"\n",
    "INPUT_VERSION = 1\n",
    "OUTPUT_VERSION = 4\n",
    "S3_INPUT_PATH = \"s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/\"\n",
    "EE_OUTPUT_VERSION = 9 \n",
    "GDAL_RASTERIZE_PATH = \"/opt/anaconda3/envs/python35/bin/gdal_rasterize\"\n",
    "\n",
    "X_DIMENSION_5MIN = 4320\n",
    "Y_DIMENSION_5MIN = 2160\n",
    "X_DIMENSION_30S = 43200\n",
    "Y_DIMENSION_30S = 21600\n",
    "\n",
    "SPATIAL_RESOLUTIONS =  [\"5min\",\"30s\"]\n",
    "PFAF_LEVELS = [\"06\",\"00\"]\n",
    "\n",
    "# ETL\n",
    "ec2_input_path = \"/volumes/data/{}/input_V{:02.0f}/\".format(SCRIPT_NAME,INPUT_VERSION)\n",
    "ec2_output_path = \"/volumes/data/{}/output_V{:02.0f}/\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "s3_output_path = \"s3://wri-projects/Aqueduct30/processData/{}/output_V{:02.0f}/\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "gcs_output_path = \"gs://aqueduct30_v01/{}/output_V{:02.0f}/\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "ee_output_path = \"projects/WRI-Aquaduct/PCRGlobWB20V{:02.0f}\".format(EE_OUTPUT_VERSION)\n",
    "\n",
    "print(\"Input s3: \" + S3_INPUT_PATH +\n",
    "      \"\\nInput ec2: \" + ec2_input_path +\n",
    "      \"\\nOutput ec2: \" + ec2_output_path +\n",
    "      \"\\nOutput S3: \" + s3_output_path +\n",
    "      \"\\nOutput gcs: \" +  gcs_output_path+\n",
    "      \"\\nOutput ee: \" + ee_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y2018M04D20 UTC 12:48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.4 |Anaconda, Inc.| (default, Nov 20 2017, 18:44:38) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, datetime, sys\n",
    "dateString = time.strftime(\"Y%YM%mD%d\")\n",
    "timeString = time.strftime(\"UTC %H:%M\")\n",
    "start = datetime.datetime.now()\n",
    "print(dateString,timeString)\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "make sure you are authorized to use AWS S3, gcs, gdal.\n",
    "\n",
    "## Origin of the WWF data\n",
    "\n",
    "The Hydrosheds data has been downloaded from the [WWF Website](http://www.hydrosheds.org/download). A login is required for larger datasets. For Aqueduct we used the Standard version without lakes. Since the download is limited to 5GB we split up the download in two batches:  \n",
    "\n",
    "1. Africa, North American Arctic, Central & South-east Asia, Australia & Oceania, Europe & Middle East\n",
    "1. Greenland, North America & Caribbean, South America, Siberia\n",
    "\n",
    "Download URLs (no longer valid)  \n",
    "[link1](http://www.hydrosheds.org/tempdownloads/hydrosheds-3926b3742a77b18974ca.zip)  \n",
    "[link2](http://www.hydrosheds.org/tempdownloads/hydrosheds-a69872e3f4059aea2434.zip)\n",
    "\n",
    "\n",
    "The data was downloaded earlier but replicated here so the latest download data would be 2017/08/03 \n",
    "\n",
    "The folders contain all levels but for this phase of Aqueduct we decided to use level 6. More information regarding this decision will be in the methodology document. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fiona\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "def etl():\n",
    "    \"\"\" Downloads and unzips files from S3 to ec2\n",
    "    \"\"\"\n",
    "    !rm -r {ec2_input_path}\n",
    "    !rm -r {ec2_output_path}\n",
    "    !mkdir -p {ec2_input_path}\n",
    "    !mkdir -p {ec2_output_path}\n",
    "    !aws s3 cp {S3_INPUT_PATH}HydrobasinsStandardAfr-Eu.zip {ec2_input_path}\n",
    "    !aws s3 cp {S3_INPUT_PATH}HydrobasinsStandardGR-SI.zip {ec2_input_path}\n",
    "    os.chdir(ec2_input_path)\n",
    "    !find . -name '*.zip' -exec unzip {} \\;\n",
    "\n",
    "def merge_shapefiles(input_meta_path,input_directory_path,search_string,output_file_path):\n",
    "    \"\"\" Merge shapefiles in directory matching regex to output.\n",
    "    -------------------------------------------------------------------------------\n",
    "\n",
    "    Args:\n",
    "        input_meta_path (string) : input path for sample shapefile with metadata.\n",
    "            The attribute table and layer name of this shapefile will be used for \n",
    "            the result.\n",
    "        input_directory_path (string) : input path of directory containing the \n",
    "            other shapefiles.\n",
    "        search_string (regex) : end of file matching this string are included in \n",
    "            the merged shapefile. \n",
    "        output_file_path (string) : Output file path for merged shapefile.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    files = os.listdir(input_directory_path)\n",
    "    meta = fiona.open(input_meta_path,encoding='UTF-8').meta\n",
    "    with fiona.open(output_file_path, 'w', **meta) as output:\n",
    "        for one_file in files:\n",
    "            if re.search(search_string,one_file):\n",
    "                print(one_file)\n",
    "                for features in fiona.open(one_file,encoding='UTF-8'):\n",
    "                    output.write(features)  \n",
    "\n",
    "                \n",
    "def rasterize_shapefiles(pfaf_level,spatial_resolution,ec2_input_path,ec2_output_path,output_version):\n",
    "    \"\"\"Rasterize shapefile using GDAL.\n",
    "    -------------------------------------------------------------------------------\n",
    "    Geotiffs are stored in the same path as input shapefile\n",
    "    \n",
    "    Args:\n",
    "        pfaf_level (string) : Pfafstetter level. Supported '06' and '00'.\n",
    "        spatial_resolution (string) : Spatial resolution. Supported '5min' and '30s'\n",
    "        ec2_input_path (string) : Path with the merged shapefiles. \n",
    "        ec2_output_path (string) : Path where geotiffs are stored.\n",
    "        output_version (integer) : Output version. \n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Rasterizing pfaf_level: {}, spatial resolution: {}\".format(pfaf_level,spatial_resolution))\n",
    "    layer_name = \"hybas_lev{}_v1c_merged_fiona_V{:02.0f}\".format(pfaf_level,OUTPUT_VERSION)\n",
    "    input_path = \"{}hybas_lev{}_v1c_merged_fiona_V{:02.0f}.shp\".format(ec2_output_path,pfaf_level,OUTPUT_VERSION)\n",
    "    output_path = \"{}hybas_lev{}_v1c_merged_fiona_{}_V{:02.0f}.tif\".format(ec2_output_path,pfaf_level,spatial_resolution,OUTPUT_VERSION)\n",
    "\n",
    "    if spatial_resolution == \"5min\":\n",
    "        x_dimension = X_DIMENSION_5MIN\n",
    "        y_dimension = Y_DIMENSION_5MIN\n",
    "    elif spatial_resolution == \"30s\":\n",
    "        x_dimension = X_DIMENSION_30S\n",
    "        y_dimension = Y_DIMENSION_30S\n",
    "    else: \n",
    "        raise(\"spatial resolution not accepted\")\n",
    "\n",
    "    if pfaf_level == \"06\":\n",
    "        field = \"PFAF_ID\"\n",
    "    elif pfaf_level == \"00\":\n",
    "        field = \"PFAF_12\"\n",
    "    else:\n",
    "        raise(\"Pfaf_level not accepted\")\n",
    "\n",
    "    command = \"{} -a {} -ot Integer64 -of GTiff -te -180 -90 180 90 -ts {} {} -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l {} -a_nodata -9999 {} {}\".format(GDAL_RASTERIZE_PATH,field,x_dimension,y_dimension,layer_name,input_path,output_path)\n",
    "    print(command)\n",
    "    response = subprocess.check_output(command,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/input_V01/': No such file or directory\n",
      "rm: cannot remove '/volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/': No such file or directory\n",
      "download: s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/HydrobasinsStandardAfr-Eu.zip to ../../../../data/Y2017M08D02_RH_Merge_HydroBasins_V02/input_V01/HydrobasinsStandardAfr-Eu.zip\n",
      "download: s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/HydrobasinsStandardGR-SI.zip to ../../../../data/Y2017M08D02_RH_Merge_HydroBasins_V02/input_V01/HydrobasinsStandardGR-SI.zip\n",
      "Archive:  ./HydrobasinsStandardGR-SI.zip\n",
      " extracting: hybas_gr_lev00_v1c.zip  \n",
      " extracting: hybas_gr_lev01-06_v1c.zip  \n",
      " extracting: hybas_gr_lev01-12_v1c.zip  \n",
      " extracting: hybas_gr_lev01_v1c.zip  \n",
      " extracting: hybas_gr_lev02_v1c.zip  \n",
      " extracting: hybas_gr_lev03_v1c.zip  \n",
      " extracting: hybas_gr_lev04_v1c.zip  \n",
      " extracting: hybas_gr_lev05_v1c.zip  \n",
      " extracting: hybas_gr_lev06_v1c.zip  \n",
      " extracting: hybas_gr_lev07_v1c.zip  \n",
      " extracting: hybas_gr_lev08_v1c.zip  \n",
      " extracting: hybas_gr_lev09_v1c.zip  \n",
      " extracting: hybas_gr_lev10_v1c.zip  \n",
      " extracting: hybas_gr_lev11_v1c.zip  \n",
      " extracting: hybas_gr_lev12_v1c.zip  \n",
      " extracting: hybas_na_lev00_v1c.zip  \n",
      " extracting: hybas_na_lev01-06_v1c.zip  \n",
      " extracting: hybas_na_lev01-12_v1c.zip  \n",
      " extracting: hybas_na_lev01_v1c.zip  \n",
      " extracting: hybas_na_lev02_v1c.zip  \n",
      " extracting: hybas_na_lev03_v1c.zip  \n",
      " extracting: hybas_na_lev04_v1c.zip  \n",
      " extracting: hybas_na_lev05_v1c.zip  \n",
      " extracting: hybas_na_lev06_v1c.zip  \n",
      " extracting: hybas_na_lev07_v1c.zip  \n",
      " extracting: hybas_na_lev08_v1c.zip  \n",
      " extracting: hybas_na_lev09_v1c.zip  \n",
      " extracting: hybas_na_lev10_v1c.zip  \n",
      " extracting: hybas_na_lev11_v1c.zip  \n",
      " extracting: hybas_na_lev12_v1c.zip  \n",
      " extracting: hybas_sa_lev00_v1c.zip  \n",
      " extracting: hybas_sa_lev01-06_v1c.zip  \n",
      " extracting: hybas_sa_lev01-12_v1c.zip  \n",
      " extracting: hybas_sa_lev01_v1c.zip  \n",
      " extracting: hybas_sa_lev02_v1c.zip  \n",
      " extracting: hybas_sa_lev03_v1c.zip  \n",
      " extracting: hybas_sa_lev04_v1c.zip  \n",
      " extracting: hybas_sa_lev05_v1c.zip  \n",
      " extracting: hybas_sa_lev06_v1c.zip  \n",
      " extracting: hybas_sa_lev07_v1c.zip  \n",
      " extracting: hybas_sa_lev08_v1c.zip  \n",
      " extracting: hybas_sa_lev09_v1c.zip  \n",
      " extracting: hybas_sa_lev10_v1c.zip  \n",
      " extracting: hybas_sa_lev11_v1c.zip  \n",
      " extracting: hybas_sa_lev12_v1c.zip  \n",
      " extracting: hybas_si_lev00_v1c.zip  \n",
      " extracting: hybas_si_lev01-06_v1c.zip  \n",
      " extracting: hybas_si_lev01-12_v1c.zip  \n",
      " extracting: hybas_si_lev01_v1c.zip  \n",
      " extracting: hybas_si_lev02_v1c.zip  \n",
      " extracting: hybas_si_lev03_v1c.zip  \n",
      " extracting: hybas_si_lev04_v1c.zip  \n",
      " extracting: hybas_si_lev05_v1c.zip  \n",
      " extracting: hybas_si_lev06_v1c.zip  \n",
      " extracting: hybas_si_lev07_v1c.zip  \n",
      " extracting: hybas_si_lev08_v1c.zip  \n",
      " extracting: hybas_si_lev09_v1c.zip  \n",
      " extracting: hybas_si_lev10_v1c.zip  \n",
      " extracting: hybas_si_lev11_v1c.zip  \n",
      " extracting: hybas_si_lev12_v1c.zip  \n",
      "Archive:  ./HydrobasinsStandardAfr-Eu.zip\n",
      " extracting: hybas_af_lev00_v1c.zip  \n",
      " extracting: hybas_af_lev01-06_v1c.zip  \n",
      " extracting: hybas_af_lev01-12_v1c.zip  \n",
      " extracting: hybas_af_lev01_v1c.zip  \n",
      " extracting: hybas_af_lev02_v1c.zip  \n",
      " extracting: hybas_af_lev03_v1c.zip  \n",
      " extracting: hybas_af_lev04_v1c.zip  \n",
      " extracting: hybas_af_lev05_v1c.zip  \n",
      " extracting: hybas_af_lev06_v1c.zip  \n",
      " extracting: hybas_af_lev07_v1c.zip  \n",
      " extracting: hybas_af_lev08_v1c.zip  \n",
      " extracting: hybas_af_lev09_v1c.zip  \n",
      " extracting: hybas_af_lev10_v1c.zip  \n",
      " extracting: hybas_af_lev11_v1c.zip  \n",
      " extracting: hybas_af_lev12_v1c.zip  \n",
      " extracting: hybas_ar_lev00_v1c.zip  \n",
      " extracting: hybas_ar_lev01-06_v1c.zip  \n",
      " extracting: hybas_ar_lev01-12_v1c.zip  \n",
      " extracting: hybas_ar_lev01_v1c.zip  \n",
      " extracting: hybas_ar_lev02_v1c.zip  \n",
      " extracting: hybas_ar_lev03_v1c.zip  \n",
      " extracting: hybas_ar_lev04_v1c.zip  \n",
      " extracting: hybas_ar_lev05_v1c.zip  \n",
      " extracting: hybas_ar_lev06_v1c.zip  \n",
      " extracting: hybas_ar_lev07_v1c.zip  \n",
      " extracting: hybas_ar_lev08_v1c.zip  \n",
      " extracting: hybas_ar_lev09_v1c.zip  \n",
      " extracting: hybas_ar_lev10_v1c.zip  \n",
      " extracting: hybas_ar_lev11_v1c.zip  \n",
      " extracting: hybas_ar_lev12_v1c.zip  \n",
      " extracting: hybas_as_lev00_v1c.zip  \n",
      " extracting: hybas_as_lev01-06_v1c.zip  \n",
      " extracting: hybas_as_lev01-12_v1c.zip  \n",
      " extracting: hybas_as_lev01_v1c.zip  \n",
      " extracting: hybas_as_lev02_v1c.zip  \n",
      " extracting: hybas_as_lev03_v1c.zip  \n",
      " extracting: hybas_as_lev04_v1c.zip  \n",
      " extracting: hybas_as_lev05_v1c.zip  \n",
      " extracting: hybas_as_lev06_v1c.zip  \n",
      " extracting: hybas_as_lev07_v1c.zip  \n",
      " extracting: hybas_as_lev08_v1c.zip  \n",
      " extracting: hybas_as_lev09_v1c.zip  \n",
      " extracting: hybas_as_lev10_v1c.zip  \n",
      " extracting: hybas_as_lev11_v1c.zip  \n",
      " extracting: hybas_as_lev12_v1c.zip  \n",
      " extracting: hybas_au_lev00_v1c.zip  \n",
      " extracting: hybas_au_lev01-06_v1c.zip  \n",
      " extracting: hybas_au_lev01-12_v1c.zip  \n",
      " extracting: hybas_au_lev01_v1c.zip  \n",
      " extracting: hybas_au_lev02_v1c.zip  \n",
      " extracting: hybas_au_lev03_v1c.zip  \n",
      " extracting: hybas_au_lev04_v1c.zip  \n",
      " extracting: hybas_au_lev05_v1c.zip  \n",
      " extracting: hybas_au_lev06_v1c.zip  \n",
      " extracting: hybas_au_lev07_v1c.zip  \n",
      " extracting: hybas_au_lev08_v1c.zip  \n",
      " extracting: hybas_au_lev09_v1c.zip  \n",
      " extracting: hybas_au_lev10_v1c.zip  \n",
      " extracting: hybas_au_lev11_v1c.zip  \n",
      " extracting: hybas_au_lev12_v1c.zip  \n",
      " extracting: hybas_eu_lev00_v1c.zip  \n",
      " extracting: hybas_eu_lev01-06_v1c.zip  \n",
      " extracting: hybas_eu_lev01-12_v1c.zip  \n",
      " extracting: hybas_eu_lev01_v1c.zip  \n",
      " extracting: hybas_eu_lev02_v1c.zip  \n",
      " extracting: hybas_eu_lev03_v1c.zip  \n",
      " extracting: hybas_eu_lev04_v1c.zip  \n",
      " extracting: hybas_eu_lev05_v1c.zip  \n",
      " extracting: hybas_eu_lev06_v1c.zip  \n",
      " extracting: hybas_eu_lev07_v1c.zip  \n",
      " extracting: hybas_eu_lev08_v1c.zip  \n",
      " extracting: hybas_eu_lev09_v1c.zip  \n",
      " extracting: hybas_eu_lev10_v1c.zip  \n",
      " extracting: hybas_eu_lev11_v1c.zip  \n",
      " extracting: hybas_eu_lev12_v1c.zip  \n"
     ]
    }
   ],
   "source": [
    "# ETL\n",
    "etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find / -name '*lev06_v1c.zip' -exec unzip -o {} \\;\n",
      "hybas_ar_lev06_v1c.shp\n",
      "hybas_na_lev06_v1c.shp\n",
      "hybas_eu_lev06_v1c.shp\n",
      "hybas_gr_lev06_v1c.shp\n",
      "hybas_au_lev06_v1c.shp\n",
      "hybas_si_lev06_v1c.shp\n",
      "hybas_sa_lev06_v1c.shp\n",
      "hybas_af_lev06_v1c.shp\n",
      "hybas_as_lev06_v1c.shp\n",
      "find / -name '*lev00_v1c.zip' -exec unzip -o {} \\;\n",
      "hybas_as_lev00_v1c.shp\n",
      "hybas_af_lev00_v1c.shp\n",
      "hybas_si_lev00_v1c.shp\n",
      "hybas_eu_lev00_v1c.shp\n",
      "hybas_sa_lev00_v1c.shp\n",
      "hybas_na_lev00_v1c.shp\n",
      "hybas_au_lev00_v1c.shp\n",
      "hybas_ar_lev00_v1c.shp\n",
      "hybas_gr_lev00_v1c.shp\n"
     ]
    }
   ],
   "source": [
    "# merging shapefiles\n",
    "for pfaf_level in PFAF_LEVELS:\n",
    "    command = \"find / -name '*lev{}_v1c.zip' -exec unzip -o {{}} \\;\".format(pfaf_level)\n",
    "    print(command)\n",
    "    response = subprocess.check_output(command,shell=True)\n",
    "    \n",
    "    input_directory_path = ec2_input_path\n",
    "    input_meta_path = '{}hybas_ar_lev{}_v1c.shp'.format(ec2_input_path,pfaf_level)\n",
    "    output_file_path = \"{}/hybas_lev{}_v1c_merged_fiona_V{:02.0f}.shp\".format(ec2_output_path,pfaf_level,OUTPUT_VERSION)\n",
    "    search_string = \"lev{}_v1c.shp$\".format(pfaf_level)\n",
    "    merge_shapefiles(input_meta_path,input_directory_path,search_string,output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also like to have rasterized versions of the shapefiles at 5min and 30s resolution (0.0833333 degrees and 0.00833333 degrees).\n",
    "Rasterizing on PFAF_ID and PFAF_12\n",
    "Layer name hybas_lev00_v1c_merged_fiona_V01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterizing pfaf_level: 06, spatial resolution: 5min\n",
      "/opt/anaconda3/envs/python35/bin/gdal_rasterize -a PFAF_ID -ot Integer64 -of GTiff -te -180 -90 180 90 -ts 4320 2160 -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l hybas_lev06_v1c_merged_fiona_V04 -a_nodata -9999 /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_V04.shp /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_5min_V04.tif\n",
      "Rasterizing pfaf_level: 00, spatial resolution: 5min\n",
      "/opt/anaconda3/envs/python35/bin/gdal_rasterize -a PFAF_12 -ot Integer64 -of GTiff -te -180 -90 180 90 -ts 4320 2160 -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l hybas_lev00_v1c_merged_fiona_V04 -a_nodata -9999 /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_V04.shp /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_5min_V04.tif\n",
      "Rasterizing pfaf_level: 06, spatial resolution: 30s\n",
      "/opt/anaconda3/envs/python35/bin/gdal_rasterize -a PFAF_ID -ot Integer64 -of GTiff -te -180 -90 180 90 -ts 43200 21600 -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l hybas_lev06_v1c_merged_fiona_V04 -a_nodata -9999 /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_V04.shp /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_30s_V04.tif\n",
      "Rasterizing pfaf_level: 00, spatial resolution: 30s\n",
      "/opt/anaconda3/envs/python35/bin/gdal_rasterize -a PFAF_12 -ot Integer64 -of GTiff -te -180 -90 180 90 -ts 43200 21600 -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l hybas_lev00_v1c_merged_fiona_V04 -a_nodata -9999 /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_V04.shp /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_30s_V04.tif\n"
     ]
    }
   ],
   "source": [
    "#Rasterizing Shapefiles\n",
    "for spatial_resolution in SPATIAL_RESOLUTIONS:\n",
    "    for pfaf_level in PFAF_LEVELS:\n",
    "        rasterize_shapefiles(pfaf_level,spatial_resolution,ec2_output_path,ec2_output_path,OUTPUT_VERSION)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev00_v1c_merged_fiona_30s_V04.tif [Content-Type=image/tiff]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev00_v1c_merged_fiona_5min_V04.tif [Content-Type=image/tiff]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev00_v1c_merged_fiona_V04.cpg [Content-Type=application/octet-stream]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev00_v1c_merged_fiona_V04.dbf [Content-Type=application/octet-stream]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev00_v1c_merged_fiona_V04.shx [Content-Type=application/x-qgis]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev00_v1c_merged_fiona_V04.prj [Content-Type=application/octet-stream]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev00_v1c_merged_fiona_V04.shp [Content-Type=application/x-qgis]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev06_v1c_merged_fiona_30s_V04.tif [Content-Type=image/tiff]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev06_v1c_merged_fiona_V04.cpg [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev06_v1c_merged_fiona_5min_V04.tif [Content-Type=image/tiff]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev06_v1c_merged_fiona_V04.dbf [Content-Type=application/octet-stream]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev06_v1c_merged_fiona_V04.prj [Content-Type=application/octet-stream]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev06_v1c_merged_fiona_V04.shp [Content-Type=application/x-qgis]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04//hybas_lev06_v1c_merged_fiona_V04.shx [Content-Type=application/x-qgis]...\n",
      "| [14/14 files][  1.8 GiB/  1.8 GiB] 100% Done  60.6 MiB/s ETA 00:00:00         \n",
      "Operation completed over 14 objects/1.8 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp \\\n",
    "{ec2_output_path}/* \\\n",
    "{gcs_output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../output_V04/hybas_lev00_v1c_merged_fiona_V04.cpg to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_V04.cpg\n",
      "upload: ../output_V04/hybas_lev00_v1c_merged_fiona_V04.prj to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_V04.prj\n",
      "upload: ../output_V04/hybas_lev06_v1c_merged_fiona_5min_V04.tif to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_5min_V04.tif\n",
      "upload: ../output_V04/hybas_lev00_v1c_merged_fiona_5min_V04.tif to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_5min_V04.tif\n",
      "upload: ../output_V04/hybas_lev06_v1c_merged_fiona_V04.cpg to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_V04.cpg\n",
      "upload: ../output_V04/hybas_lev00_v1c_merged_fiona_V04.shx to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_V04.shx\n",
      "upload: ../output_V04/hybas_lev06_v1c_merged_fiona_V04.dbf to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_V04.dbf\n",
      "upload: ../output_V04/hybas_lev06_v1c_merged_fiona_V04.prj to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_V04.prj\n",
      "upload: ../output_V04/hybas_lev06_v1c_merged_fiona_V04.shx to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_V04.shx\n",
      "upload: ../output_V04/hybas_lev06_v1c_merged_fiona_30s_V04.tif to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_30s_V04.tif\n",
      "upload: ../output_V04/hybas_lev00_v1c_merged_fiona_30s_V04.tif to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_30s_V04.tif\n",
      "upload: ../output_V04/hybas_lev00_v1c_merged_fiona_V04.dbf to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_V04.dbf\n",
      "upload: ../output_V04/hybas_lev06_v1c_merged_fiona_V04.shp to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev06_v1c_merged_fiona_V04.shp\n",
      "upload: ../output_V04/hybas_lev00_v1c_merged_fiona_V04.shp to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V02/output_V04/hybas_lev00_v1c_merged_fiona_V04.shp\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {ec2_output_path} {s3_output_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:43:42.915611\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Runs:  \n",
    "0:43:42.915611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 35",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
