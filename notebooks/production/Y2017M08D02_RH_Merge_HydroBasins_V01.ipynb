{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input s3: s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/\n",
      "Input ec2: /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/\n",
      "Output ec2: /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output_V01/\n",
      "Output S3: s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V01/output_V01/\n",
      "Output GCS: gs://aqueduct30_v01/Y2017M08D02_RH_Merge_HydroBasins_V01/output_V01/\n",
      "Output ee: projects/WRI-Aquaduct/PCRGlobWB20V09\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Merge WWF's HydroBasins\n",
    "-------------------------------------------------------------------------------\n",
    "copy the relevant files from raw ro process and merge the shapefiles of level 6\n",
    "and level 00. Uploads to S3 + GCS and ingest into EE. \n",
    "\n",
    "Author: Rutger Hofste\n",
    "Date: 20170802\n",
    "Kernel: python35\n",
    "Docker: rutgerhofste/gisdocker:ubuntu16.04\n",
    "\n",
    "Args:\n",
    "\n",
    "    SCRIPT_NAME (string) : Script name\n",
    "    S3_INPUT_PATH (string) : Name of script used as input. \n",
    "\n",
    "Returns:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Input Parameters\n",
    "\n",
    "SCRIPT_NAME = \"Y2017M08D02_RH_Merge_HydroBasins_V01\"\n",
    "OUTPUT_VERSION = 1\n",
    "\n",
    "S3_INPUT_PATH = \"s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/\"\n",
    "\n",
    "GCS_OUTPUT = \"gs://aqueduct30_v01/Y2017M08D02_RH_Merge_HydroBasins_V01/output/\"\n",
    "EE_OUTPUT_VERSION = 9 \n",
    "\n",
    "\n",
    "# ETL\n",
    "ec2_input_path = \"/volumes/data/{}/input/\".format(SCRIPT_NAME)\n",
    "ec2_output_path = \"/volumes/data/{}/output_V{:02.0f}/\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "s3_output_path = \"s3://wri-projects/Aqueduct30/processData/{}/output_V{:02.0f}/\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "gcs_output_path = \"gs://aqueduct30_v01/{}/output_V{:02.0f}/\".format(SCRIPT_NAME,OUTPUT_VERSION)\n",
    "ee_output_path = \"projects/WRI-Aquaduct/PCRGlobWB20V{:02.0f}\".format(EE_OUTPUT_VERSION)\n",
    "\n",
    "lonSize5min = 4320\n",
    "latSize5min = 2160\n",
    "lonSize30s = 43200 \n",
    "latSize30s = 21600\n",
    "\n",
    "\n",
    "print(\"Input s3: \" + S3_INPUT_PATH +\n",
    "      \"\\nInput ec2: \" + ec2_input_path +\n",
    "      \"\\nOutput ec2: \" + ec2_output_path +\n",
    "      \"\\nOutput S3: \" + s3_output_path +\n",
    "      \"\\nOutput gcs: \" +  gcs_output_path+\n",
    "      \"\\nOutput ee: \" + ee_output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "make sure you are authorized to use AWS S3\n",
    "\n",
    "## Origin of the WWF data\n",
    "\n",
    "The Hydrosheds data has been downloaded from the [WWF Website](http://www.hydrosheds.org/download). A login is required for larger datasets. For Aqueduct we used the Standard version without lakes. Since the download is limited to 5GB we split up the download in two batches:  \n",
    "\n",
    "1. Africa, North American Arctic, Central & South-east Asia, Australia & Oceania, Europe & Middle East\n",
    "1. Greenland, North America & Caribbean, South America, Siberia\n",
    "\n",
    "Download URLs (no longer valid)  \n",
    "[link1](http://www.hydrosheds.org/tempdownloads/hydrosheds-3926b3742a77b18974ca.zip)  \n",
    "[link2](http://www.hydrosheds.org/tempdownloads/hydrosheds-a69872e3f4059aea2434.zip)\n",
    "\n",
    "\n",
    "The data was downloaded earlier but replicated here so the latest download data would be 2017/08/03 \n",
    "\n",
    "The folders contain all levels but for this phase of Aqueduct we decided to use level 6. More information regarding this decision will be in the methodology document. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script\n",
    "copy the files from the raw data folder to the process data folder. The raw data folder contains pristine or untouched data and should not be used as a working directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S3_INPUT_PATH = \"s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/\"\n",
    "S3_PATH = \"s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V01/input/\"\n",
    "S3_OUTPUT_PATH = \"s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V01/output/\"\n",
    "EC2_INPUT_PATH = \"/volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/\"\n",
    "EC2_OUTPUT_PATH = \"/volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/\"\n",
    "GCS_OUTPUT = \"gs://aqueduct30_v01/Y2017M08D02_RH_Merge_HydroBasins_V01/output/\"\n",
    "EE_OUTPUT_PATH = \"projects/WRI-Aquaduct/PCRGlobWB20V07/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fiona\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitKey(key):\n",
    "    # will yield the root file code and extension of a set of keys\n",
    "    prefix, extension = key.split(\".\")\n",
    "    fileName = prefix.split(\"/\")[-1]\n",
    "    values = re.split(\"_|-\", fileName)\n",
    "    keyz = [\"indicator\",\"spatial_resolution\",\"WWFversion\",\"geographic_range\",\"library\",\"spatial_resolution\",\"version\"]\n",
    "    outDict = dict(zip(keyz, values))\n",
    "    outDict[\"fileName\"]=fileName\n",
    "    outDict[\"extension\"]=extension\n",
    "    return outDict\n",
    "\n",
    "def uploadEE(index,row):\n",
    "    target = EE_OUTPUT_PATH + row.fileName\n",
    "    source = GCS_OUTPUT + row.fileName + \".\" + row.extension\n",
    "    \n",
    "    metadata = \"--nodata_value=%s -p wwfversion=%s -p extension=%s -p filename=%s -p geographic_range=%s -p library=%s -p spatial_resolution=%s -p version=%s -p ingested_by=%s -p exportdescription=%s -p units=%s\" %(row.nodata,row.WWFversion,row.extension,row.fileName,row.geographic_range, row.library, row.spatial_resolution, row.version, row.ingested_by, row.exportdescription, row.units)\n",
    "    command = \"/opt/anaconda3/bin/earthengine upload image --asset_id %s %s %s\" % (target, source,metadata)\n",
    "    try:\n",
    "        response = subprocess.check_output(command, shell=True)\n",
    "        outDict = {\"command\":command,\"response\":response,\"error\":0}\n",
    "        df_errors2 = pd.DataFrame(outDict,index=[index])\n",
    "        pass\n",
    "    except:\n",
    "        try:\n",
    "            outDict = {\"command\":command,\"response\":response,\"error\":1}\n",
    "        except:\n",
    "            outDict = {\"command\":command,\"response\":-9999,\"error\":2}\n",
    "        df_errors2 = pd.DataFrame(outDict,index=[index])\n",
    "        print(\"error\")\n",
    "    return df_errors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/HydrobasinsStandardAfr-Eu.zip to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V01/input/HydrobasinsStandardAfr-Eu.zip\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {S3_INPUT_PATH}HydrobasinsStandardAfr-Eu.zip {S3_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://wri-projects/Aqueduct30/rawData/WWF/HydroSheds30sComplete/HydrobasinsStandardGR-SI.zip to s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V01/input/HydrobasinsStandardGR-SI.zip\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {S3_INPUT_PATH}HydrobasinsStandardGR-SI.zip {S3_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p {EC2_INPUT_PATH}\n",
    "!mkdir -p {EC2_OUTPUT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V01/input/HydrobasinsStandardGR-SI.zip to ../../../../data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/HydrobasinsStandardGR-SI.zip\n",
      "download: s3://wri-projects/Aqueduct30/processData/Y2017M08D02_RH_Merge_HydroBasins_V01/input/HydrobasinsStandardAfr-Eu.zip to ../../../../data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/HydrobasinsStandardAfr-Eu.zip\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {S3_PATH} {EC2_INPUT_PATH} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip shapefiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(EC2_INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./HydrobasinsStandardAfr-Eu.zip\n",
      " extracting: hybas_af_lev00_v1c.zip  \n",
      " extracting: hybas_af_lev01-06_v1c.zip  \n",
      " extracting: hybas_af_lev01-12_v1c.zip  \n",
      " extracting: hybas_af_lev01_v1c.zip  \n",
      " extracting: hybas_af_lev02_v1c.zip  \n",
      " extracting: hybas_af_lev03_v1c.zip  \n",
      " extracting: hybas_af_lev04_v1c.zip  \n",
      " extracting: hybas_af_lev05_v1c.zip  \n",
      " extracting: hybas_af_lev06_v1c.zip  \n",
      " extracting: hybas_af_lev07_v1c.zip  \n",
      " extracting: hybas_af_lev08_v1c.zip  \n",
      " extracting: hybas_af_lev09_v1c.zip  \n",
      " extracting: hybas_af_lev10_v1c.zip  \n",
      " extracting: hybas_af_lev11_v1c.zip  \n",
      " extracting: hybas_af_lev12_v1c.zip  \n",
      " extracting: hybas_ar_lev00_v1c.zip  \n",
      " extracting: hybas_ar_lev01-06_v1c.zip  \n",
      " extracting: hybas_ar_lev01-12_v1c.zip  \n",
      " extracting: hybas_ar_lev01_v1c.zip  \n",
      " extracting: hybas_ar_lev02_v1c.zip  \n",
      " extracting: hybas_ar_lev03_v1c.zip  \n",
      " extracting: hybas_ar_lev04_v1c.zip  \n",
      " extracting: hybas_ar_lev05_v1c.zip  \n",
      " extracting: hybas_ar_lev06_v1c.zip  \n",
      " extracting: hybas_ar_lev07_v1c.zip  \n",
      " extracting: hybas_ar_lev08_v1c.zip  \n",
      " extracting: hybas_ar_lev09_v1c.zip  \n",
      " extracting: hybas_ar_lev10_v1c.zip  \n",
      " extracting: hybas_ar_lev11_v1c.zip  \n",
      " extracting: hybas_ar_lev12_v1c.zip  \n",
      " extracting: hybas_as_lev00_v1c.zip  \n",
      " extracting: hybas_as_lev01-06_v1c.zip  \n",
      " extracting: hybas_as_lev01-12_v1c.zip  \n",
      " extracting: hybas_as_lev01_v1c.zip  \n",
      " extracting: hybas_as_lev02_v1c.zip  \n",
      " extracting: hybas_as_lev03_v1c.zip  \n",
      " extracting: hybas_as_lev04_v1c.zip  \n",
      " extracting: hybas_as_lev05_v1c.zip  \n",
      " extracting: hybas_as_lev06_v1c.zip  \n",
      " extracting: hybas_as_lev07_v1c.zip  \n",
      " extracting: hybas_as_lev08_v1c.zip  \n",
      " extracting: hybas_as_lev09_v1c.zip  \n",
      " extracting: hybas_as_lev10_v1c.zip  \n",
      " extracting: hybas_as_lev11_v1c.zip  \n",
      " extracting: hybas_as_lev12_v1c.zip  \n",
      " extracting: hybas_au_lev00_v1c.zip  \n",
      " extracting: hybas_au_lev01-06_v1c.zip  \n",
      " extracting: hybas_au_lev01-12_v1c.zip  \n",
      " extracting: hybas_au_lev01_v1c.zip  \n",
      " extracting: hybas_au_lev02_v1c.zip  \n",
      " extracting: hybas_au_lev03_v1c.zip  \n",
      " extracting: hybas_au_lev04_v1c.zip  \n",
      " extracting: hybas_au_lev05_v1c.zip  \n",
      " extracting: hybas_au_lev06_v1c.zip  \n",
      " extracting: hybas_au_lev07_v1c.zip  \n",
      " extracting: hybas_au_lev08_v1c.zip  \n",
      " extracting: hybas_au_lev09_v1c.zip  \n",
      " extracting: hybas_au_lev10_v1c.zip  \n",
      " extracting: hybas_au_lev11_v1c.zip  \n",
      " extracting: hybas_au_lev12_v1c.zip  \n",
      " extracting: hybas_eu_lev00_v1c.zip  \n",
      " extracting: hybas_eu_lev01-06_v1c.zip  \n",
      " extracting: hybas_eu_lev01-12_v1c.zip  \n",
      " extracting: hybas_eu_lev01_v1c.zip  \n",
      " extracting: hybas_eu_lev02_v1c.zip  \n",
      " extracting: hybas_eu_lev03_v1c.zip  \n",
      " extracting: hybas_eu_lev04_v1c.zip  \n",
      " extracting: hybas_eu_lev05_v1c.zip  \n",
      " extracting: hybas_eu_lev06_v1c.zip  \n",
      " extracting: hybas_eu_lev07_v1c.zip  \n",
      " extracting: hybas_eu_lev08_v1c.zip  \n",
      " extracting: hybas_eu_lev09_v1c.zip  \n",
      " extracting: hybas_eu_lev10_v1c.zip  \n",
      " extracting: hybas_eu_lev11_v1c.zip  \n",
      " extracting: hybas_eu_lev12_v1c.zip  \n",
      "Archive:  ./HydrobasinsStandardGR-SI.zip\n",
      " extracting: hybas_gr_lev00_v1c.zip  \n",
      " extracting: hybas_gr_lev01-06_v1c.zip  \n",
      " extracting: hybas_gr_lev01-12_v1c.zip  \n",
      " extracting: hybas_gr_lev01_v1c.zip  \n",
      " extracting: hybas_gr_lev02_v1c.zip  \n",
      " extracting: hybas_gr_lev03_v1c.zip  \n",
      " extracting: hybas_gr_lev04_v1c.zip  \n",
      " extracting: hybas_gr_lev05_v1c.zip  \n",
      " extracting: hybas_gr_lev06_v1c.zip  \n",
      " extracting: hybas_gr_lev07_v1c.zip  \n",
      " extracting: hybas_gr_lev08_v1c.zip  \n",
      " extracting: hybas_gr_lev09_v1c.zip  \n",
      " extracting: hybas_gr_lev10_v1c.zip  \n",
      " extracting: hybas_gr_lev11_v1c.zip  \n",
      " extracting: hybas_gr_lev12_v1c.zip  \n",
      " extracting: hybas_na_lev00_v1c.zip  \n",
      " extracting: hybas_na_lev01-06_v1c.zip  \n",
      " extracting: hybas_na_lev01-12_v1c.zip  \n",
      " extracting: hybas_na_lev01_v1c.zip  \n",
      " extracting: hybas_na_lev02_v1c.zip  \n",
      " extracting: hybas_na_lev03_v1c.zip  \n",
      " extracting: hybas_na_lev04_v1c.zip  \n",
      " extracting: hybas_na_lev05_v1c.zip  \n",
      " extracting: hybas_na_lev06_v1c.zip  \n",
      " extracting: hybas_na_lev07_v1c.zip  \n",
      " extracting: hybas_na_lev08_v1c.zip  \n",
      " extracting: hybas_na_lev09_v1c.zip  \n",
      " extracting: hybas_na_lev10_v1c.zip  \n",
      " extracting: hybas_na_lev11_v1c.zip  \n",
      " extracting: hybas_na_lev12_v1c.zip  \n",
      " extracting: hybas_sa_lev00_v1c.zip  \n",
      " extracting: hybas_sa_lev01-06_v1c.zip  \n",
      " extracting: hybas_sa_lev01-12_v1c.zip  \n",
      " extracting: hybas_sa_lev01_v1c.zip  \n",
      " extracting: hybas_sa_lev02_v1c.zip  \n",
      " extracting: hybas_sa_lev03_v1c.zip  \n",
      " extracting: hybas_sa_lev04_v1c.zip  \n",
      " extracting: hybas_sa_lev05_v1c.zip  \n",
      " extracting: hybas_sa_lev06_v1c.zip  \n",
      " extracting: hybas_sa_lev07_v1c.zip  \n",
      " extracting: hybas_sa_lev08_v1c.zip  \n",
      " extracting: hybas_sa_lev09_v1c.zip  \n",
      " extracting: hybas_sa_lev10_v1c.zip  \n",
      " extracting: hybas_sa_lev11_v1c.zip  \n",
      " extracting: hybas_sa_lev12_v1c.zip  \n",
      " extracting: hybas_si_lev00_v1c.zip  \n",
      " extracting: hybas_si_lev01-06_v1c.zip  \n",
      " extracting: hybas_si_lev01-12_v1c.zip  \n",
      " extracting: hybas_si_lev01_v1c.zip  \n",
      " extracting: hybas_si_lev02_v1c.zip  \n",
      " extracting: hybas_si_lev03_v1c.zip  \n",
      " extracting: hybas_si_lev04_v1c.zip  \n",
      " extracting: hybas_si_lev05_v1c.zip  \n",
      " extracting: hybas_si_lev06_v1c.zip  \n",
      " extracting: hybas_si_lev07_v1c.zip  \n",
      " extracting: hybas_si_lev08_v1c.zip  \n",
      " extracting: hybas_si_lev09_v1c.zip  \n",
      " extracting: hybas_si_lev10_v1c.zip  \n",
      " extracting: hybas_si_lev11_v1c.zip  \n",
      " extracting: hybas_si_lev12_v1c.zip  \n"
     ]
    }
   ],
   "source": [
    "!find . -name '*.zip' -exec unzip {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_au_lev06_v1c.zip\n",
      "  inflating: hybas_au_lev06_v1c.dbf  \n",
      "  inflating: hybas_au_lev06_v1c.prj  \n",
      "  inflating: hybas_au_lev06_v1c.sbn  \n",
      "  inflating: hybas_au_lev06_v1c.sbx  \n",
      "  inflating: hybas_au_lev06_v1c.shp  \n",
      "  inflating: hybas_au_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_au_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_na_lev06_v1c.zip\n",
      "  inflating: hybas_na_lev06_v1c.dbf  \n",
      "  inflating: hybas_na_lev06_v1c.prj  \n",
      "  inflating: hybas_na_lev06_v1c.sbn  \n",
      "  inflating: hybas_na_lev06_v1c.sbx  \n",
      "  inflating: hybas_na_lev06_v1c.shp  \n",
      "  inflating: hybas_na_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_na_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_gr_lev06_v1c.zip\n",
      "  inflating: hybas_gr_lev06_v1c.dbf  \n",
      "  inflating: hybas_gr_lev06_v1c.prj  \n",
      "  inflating: hybas_gr_lev06_v1c.sbn  \n",
      "  inflating: hybas_gr_lev06_v1c.sbx  \n",
      "  inflating: hybas_gr_lev06_v1c.shp  \n",
      "  inflating: hybas_gr_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_gr_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_as_lev06_v1c.zip\n",
      "  inflating: hybas_as_lev06_v1c.dbf  \n",
      "  inflating: hybas_as_lev06_v1c.prj  \n",
      "  inflating: hybas_as_lev06_v1c.sbn  \n",
      "  inflating: hybas_as_lev06_v1c.sbx  \n",
      "  inflating: hybas_as_lev06_v1c.shp  \n",
      "  inflating: hybas_as_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_as_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_si_lev06_v1c.zip\n",
      "  inflating: hybas_si_lev06_v1c.dbf  \n",
      "  inflating: hybas_si_lev06_v1c.prj  \n",
      "  inflating: hybas_si_lev06_v1c.sbn  \n",
      "  inflating: hybas_si_lev06_v1c.sbx  \n",
      "  inflating: hybas_si_lev06_v1c.shp  \n",
      "  inflating: hybas_si_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_si_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_af_lev06_v1c.zip\n",
      "  inflating: hybas_af_lev06_v1c.dbf  \n",
      "  inflating: hybas_af_lev06_v1c.prj  \n",
      "  inflating: hybas_af_lev06_v1c.sbn  \n",
      "  inflating: hybas_af_lev06_v1c.sbx  \n",
      "  inflating: hybas_af_lev06_v1c.shp  \n",
      "  inflating: hybas_af_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_af_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_eu_lev06_v1c.zip\n",
      "  inflating: hybas_eu_lev06_v1c.dbf  \n",
      "  inflating: hybas_eu_lev06_v1c.prj  \n",
      "  inflating: hybas_eu_lev06_v1c.sbn  \n",
      "  inflating: hybas_eu_lev06_v1c.sbx  \n",
      "  inflating: hybas_eu_lev06_v1c.shp  \n",
      "  inflating: hybas_eu_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_eu_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_ar_lev06_v1c.zip\n",
      "  inflating: hybas_ar_lev06_v1c.dbf  \n",
      "  inflating: hybas_ar_lev06_v1c.prj  \n",
      "  inflating: hybas_ar_lev06_v1c.sbn  \n",
      "  inflating: hybas_ar_lev06_v1c.sbx  \n",
      "  inflating: hybas_ar_lev06_v1c.shp  \n",
      "  inflating: hybas_ar_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_ar_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_sa_lev06_v1c.zip\n",
      "  inflating: hybas_sa_lev06_v1c.dbf  \n",
      "  inflating: hybas_sa_lev06_v1c.prj  \n",
      "  inflating: hybas_sa_lev06_v1c.sbn  \n",
      "  inflating: hybas_sa_lev06_v1c.sbx  \n",
      "  inflating: hybas_sa_lev06_v1c.shp  \n",
      "  inflating: hybas_sa_lev06_v1c.shp.xml  \n",
      "  inflating: hybas_sa_lev06_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_ar_lev00_v1c.zip\n",
      "  inflating: hybas_ar_lev00_v1c.dbf  \n",
      "  inflating: hybas_ar_lev00_v1c.prj  \n",
      "  inflating: hybas_ar_lev00_v1c.sbn  \n",
      "  inflating: hybas_ar_lev00_v1c.sbx  \n",
      "  inflating: hybas_ar_lev00_v1c.shp  \n",
      "  inflating: hybas_ar_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_ar_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_as_lev00_v1c.zip\n",
      "  inflating: hybas_as_lev00_v1c.dbf  \n",
      "  inflating: hybas_as_lev00_v1c.prj  \n",
      "  inflating: hybas_as_lev00_v1c.sbn  \n",
      "  inflating: hybas_as_lev00_v1c.sbx  \n",
      "  inflating: hybas_as_lev00_v1c.shp  \n",
      "  inflating: hybas_as_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_as_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_au_lev00_v1c.zip\n",
      "  inflating: hybas_au_lev00_v1c.dbf  \n",
      "  inflating: hybas_au_lev00_v1c.prj  \n",
      "  inflating: hybas_au_lev00_v1c.sbn  \n",
      "  inflating: hybas_au_lev00_v1c.sbx  \n",
      "  inflating: hybas_au_lev00_v1c.shp  \n",
      "  inflating: hybas_au_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_au_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_si_lev00_v1c.zip\n",
      "  inflating: hybas_si_lev00_v1c.dbf  \n",
      "  inflating: hybas_si_lev00_v1c.prj  \n",
      "  inflating: hybas_si_lev00_v1c.sbn  \n",
      "  inflating: hybas_si_lev00_v1c.sbx  \n",
      "  inflating: hybas_si_lev00_v1c.shp  \n",
      "  inflating: hybas_si_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_si_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_sa_lev00_v1c.zip\n",
      "  inflating: hybas_sa_lev00_v1c.dbf  \n",
      "  inflating: hybas_sa_lev00_v1c.prj  \n",
      "  inflating: hybas_sa_lev00_v1c.sbn  \n",
      "  inflating: hybas_sa_lev00_v1c.sbx  \n",
      "  inflating: hybas_sa_lev00_v1c.shp  \n",
      "  inflating: hybas_sa_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_sa_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_gr_lev00_v1c.zip\n",
      "  inflating: hybas_gr_lev00_v1c.dbf  \n",
      "  inflating: hybas_gr_lev00_v1c.prj  \n",
      "  inflating: hybas_gr_lev00_v1c.sbn  \n",
      "  inflating: hybas_gr_lev00_v1c.sbx  \n",
      "  inflating: hybas_gr_lev00_v1c.shp  \n",
      "  inflating: hybas_gr_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_gr_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_af_lev00_v1c.zip\n",
      "  inflating: hybas_af_lev00_v1c.dbf  \n",
      "  inflating: hybas_af_lev00_v1c.prj  \n",
      "  inflating: hybas_af_lev00_v1c.sbn  \n",
      "  inflating: hybas_af_lev00_v1c.sbx  \n",
      "  inflating: hybas_af_lev00_v1c.shp  \n",
      "  inflating: hybas_af_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_af_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_na_lev00_v1c.zip\n",
      "  inflating: hybas_na_lev00_v1c.dbf  \n",
      "  inflating: hybas_na_lev00_v1c.prj  \n",
      "  inflating: hybas_na_lev00_v1c.sbn  \n",
      "  inflating: hybas_na_lev00_v1c.sbx  \n",
      "  inflating: hybas_na_lev00_v1c.shp  \n",
      "  inflating: hybas_na_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_na_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n",
      "Archive:  /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/input/hybas_eu_lev00_v1c.zip\n",
      "  inflating: hybas_eu_lev00_v1c.dbf  \n",
      "  inflating: hybas_eu_lev00_v1c.prj  \n",
      "  inflating: hybas_eu_lev00_v1c.sbn  \n",
      "  inflating: hybas_eu_lev00_v1c.sbx  \n",
      "  inflating: hybas_eu_lev00_v1c.shp  \n",
      "  inflating: hybas_eu_lev00_v1c.shp.xml  \n",
      "  inflating: hybas_eu_lev00_v1c.shx  \n",
      "  inflating: HydroBASINS_TechDoc_v1c.pdf  \n"
     ]
    }
   ],
   "source": [
    "!find / -name '*lev06_v1c.zip' -exec unzip -o {} \\;\n",
    "!find / -name '*lev00_v1c.zip' -exec unzip -o {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir(EC2_INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybas_sa_lev06_v1c.shp\n",
      "hybas_as_lev06_v1c.shp\n",
      "hybas_eu_lev06_v1c.shp\n",
      "hybas_af_lev06_v1c.shp\n",
      "hybas_au_lev06_v1c.shp\n",
      "hybas_na_lev06_v1c.shp\n",
      "hybas_ar_lev06_v1c.shp\n",
      "hybas_gr_lev06_v1c.shp\n",
      "hybas_si_lev06_v1c.shp\n"
     ]
    }
   ],
   "source": [
    "meta = fiona.open('hybas_ar_lev06_v1c.shp',encoding='UTF-8').meta\n",
    "with fiona.open(EC2_OUTPUT_PATH+\"/hybas_lev06_v1c_merged_fiona_V01.shp\", 'w', **meta) as output:\n",
    "    for oneFile in files:    \n",
    "        if oneFile.endswith(\"lev06_v1c.shp\"):\n",
    "            print(oneFile)\n",
    "            for features in fiona.open(oneFile,encoding='UTF-8'):\n",
    "                output.write(features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybas_as_lev00_v1c.shp\n",
      "hybas_au_lev00_v1c.shp\n",
      "hybas_ar_lev00_v1c.shp\n",
      "hybas_na_lev00_v1c.shp\n",
      "hybas_si_lev00_v1c.shp\n",
      "hybas_gr_lev00_v1c.shp\n",
      "hybas_sa_lev00_v1c.shp\n",
      "hybas_eu_lev00_v1c.shp\n",
      "hybas_af_lev00_v1c.shp\n"
     ]
    }
   ],
   "source": [
    "meta = fiona.open('hybas_ar_lev00_v1c.shp').meta\n",
    "with fiona.open(EC2_OUTPUT_PATH+\"/hybas_lev00_v1c_merged_fiona_V01.shp\", 'w', **meta) as output:\n",
    "    for oneFile in files:    \n",
    "        if oneFile.endswith(\"lev00_v1c.shp\"):\n",
    "            print(oneFile)\n",
    "            for features in fiona.open(oneFile):\n",
    "                output.write(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also like to have rasterized versions of the shapefiles at 5min and 30s resolution (0.0833333 degrees and 0.00833333 degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasterizing on PFAF_ID and PFAF_12\n",
    "Layer name hybas_lev00_v1c_merged_fiona_V01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commands =[]\n",
    "commands.append(\"gdal_rasterize -a PFAF_ID -ot Integer64 -of GTiff -te -180 -90 180 90 -ts %s %s -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l hybas_lev06_v1c_merged_fiona_V01 -a_nodata -9999 %shybas_lev06_v1c_merged_fiona_V01.shp /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev06_v1c_merged_fiona__5min_V01.tif\" %(lonSize5min,latSize5min,EC2_OUTPUT_PATH))\n",
    "commands.append(\"gdal_rasterize -a PFAF_ID -ot Integer64 -of GTiff -te -180 -90 180 90 -ts %s %s -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l hybas_lev06_v1c_merged_fiona_V01 -a_nodata -9999 %shybas_lev06_v1c_merged_fiona_V01.shp /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev06_v1c_merged_fiona_30s_V01.tif\" %(lonSize30s,latSize30s,EC2_OUTPUT_PATH))\n",
    "commands.append(\"gdal_rasterize -a PFAF_12 -ot Integer64 -of GTiff -te -180 -90 180 90 -ts %s %s -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l hybas_lev00_v1c_merged_fiona_V01 -a_nodata -9999 %shybas_lev00_v1c_merged_fiona_V01.shp /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev00_v1c_merged_fiona_5min_V01.tif\" %(lonSize5min,latSize5min,EC2_OUTPUT_PATH))\n",
    "commands.append(\"gdal_rasterize -a PFAF_12 -ot Integer64 -of GTiff -te -180 -90 180 90 -ts %s %s -co COMPRESS=DEFLATE -co PREDICTOR=1 -co ZLEVEL=6 -l hybas_lev00_v1c_merged_fiona_V01 -a_nodata -9999 %shybas_lev00_v1c_merged_fiona_V01.shp /volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev00_v1c_merged_fiona_30s_V01.tif\" %(lonSize30s,latSize30s,EC2_OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasterizing (takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for command in commands:\n",
    "    #print(command)\n",
    "    response = subprocess.check_output(command,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp \\\n",
    "{EC2_OUTPUT_PATH} \\\n",
    "{S3_OUTPUT_PATH} \\\n",
    "--recursive \\\n",
    "--quiet \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev00_v1c_merged_fiona_30s_V01.tif [Content-Type=image/tiff]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev00_v1c_merged_fiona_5min_V01.tif [Content-Type=image/tiff]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev06_v1c_merged_fiona_30s_V01.tif [Content-Type=image/tiff]...\n",
      "Copying file:///volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev06_v1c_merged_fiona_5min_V01.tif [Content-Type=image/tiff]...\n",
      "| [4/4 files][116.7 MiB/116.7 MiB] 100% Done                                    \n",
      "Operation completed over 4 objects/116.7 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp \\\n",
    "/volumes/data/Y2017M08D02_RH_Merge_HydroBasins_V01/output/*.tif \\\n",
    "{GCS_OUTPUT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://aqueduct30_v01/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev00_v1c_merged_fiona_30s_V01.tif', 'gs://aqueduct30_v01/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev00_v1c_merged_fiona_5min_V01.tif', 'gs://aqueduct30_v01/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev06_v1c_merged_fiona_30s_V01.tif', 'gs://aqueduct30_v01/Y2017M08D02_RH_Merge_HydroBasins_V01/output/hybas_lev06_v1c_merged_fiona_5min_V01.tif']\n"
     ]
    }
   ],
   "source": [
    "command = (\"/opt/google-cloud-sdk/bin/gsutil ls %s\") %(GCS_OUTPUT)\n",
    "keys = subprocess.check_output(command,shell=True)\n",
    "keys = keys.decode('UTF-8').splitlines()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "i = 0\n",
    "for key in keys:\n",
    "    i = i+1\n",
    "    outDict = splitKey(key)\n",
    "    df2 = pd.DataFrame(outDict,index=[i])\n",
    "    df = df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WWFversion</th>\n",
       "      <th>extension</th>\n",
       "      <th>fileName</th>\n",
       "      <th>geographic_range</th>\n",
       "      <th>indicator</th>\n",
       "      <th>library</th>\n",
       "      <th>spatial_resolution</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v1c</td>\n",
       "      <td>tif</td>\n",
       "      <td>hybas_lev00_v1c_merged_fiona_30s_V01</td>\n",
       "      <td>merged</td>\n",
       "      <td>hybas</td>\n",
       "      <td>fiona</td>\n",
       "      <td>30s</td>\n",
       "      <td>V01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v1c</td>\n",
       "      <td>tif</td>\n",
       "      <td>hybas_lev00_v1c_merged_fiona_5min_V01</td>\n",
       "      <td>merged</td>\n",
       "      <td>hybas</td>\n",
       "      <td>fiona</td>\n",
       "      <td>5min</td>\n",
       "      <td>V01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v1c</td>\n",
       "      <td>tif</td>\n",
       "      <td>hybas_lev06_v1c_merged_fiona_30s_V01</td>\n",
       "      <td>merged</td>\n",
       "      <td>hybas</td>\n",
       "      <td>fiona</td>\n",
       "      <td>30s</td>\n",
       "      <td>V01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v1c</td>\n",
       "      <td>tif</td>\n",
       "      <td>hybas_lev06_v1c_merged_fiona_5min_V01</td>\n",
       "      <td>merged</td>\n",
       "      <td>hybas</td>\n",
       "      <td>fiona</td>\n",
       "      <td>5min</td>\n",
       "      <td>V01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WWFversion extension                               fileName  \\\n",
       "1        v1c       tif   hybas_lev00_v1c_merged_fiona_30s_V01   \n",
       "2        v1c       tif  hybas_lev00_v1c_merged_fiona_5min_V01   \n",
       "3        v1c       tif   hybas_lev06_v1c_merged_fiona_30s_V01   \n",
       "4        v1c       tif  hybas_lev06_v1c_merged_fiona_5min_V01   \n",
       "\n",
       "  geographic_range indicator library spatial_resolution version  \n",
       "1           merged     hybas   fiona                30s     V01  \n",
       "2           merged     hybas   fiona               5min     V01  \n",
       "3           merged     hybas   fiona                30s     V01  \n",
       "4           merged     hybas   fiona               5min     V01  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"nodata\"] = -9999\n",
    "df[\"ingested_by\"] =\"RutgerHofste\"\n",
    "df[\"exportdescription\"] = df[\"indicator\"]\n",
    "df[\"units\"] = \"PFAF_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WWFversion</th>\n",
       "      <th>extension</th>\n",
       "      <th>fileName</th>\n",
       "      <th>geographic_range</th>\n",
       "      <th>indicator</th>\n",
       "      <th>library</th>\n",
       "      <th>spatial_resolution</th>\n",
       "      <th>version</th>\n",
       "      <th>nodata</th>\n",
       "      <th>ingested_by</th>\n",
       "      <th>exportdescription</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v1c</td>\n",
       "      <td>tif</td>\n",
       "      <td>hybas_lev00_v1c_merged_fiona_30s_V01</td>\n",
       "      <td>merged</td>\n",
       "      <td>hybas</td>\n",
       "      <td>fiona</td>\n",
       "      <td>30s</td>\n",
       "      <td>V01</td>\n",
       "      <td>-9999</td>\n",
       "      <td>RutgerHofste</td>\n",
       "      <td>hybas</td>\n",
       "      <td>PFAF_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v1c</td>\n",
       "      <td>tif</td>\n",
       "      <td>hybas_lev00_v1c_merged_fiona_5min_V01</td>\n",
       "      <td>merged</td>\n",
       "      <td>hybas</td>\n",
       "      <td>fiona</td>\n",
       "      <td>5min</td>\n",
       "      <td>V01</td>\n",
       "      <td>-9999</td>\n",
       "      <td>RutgerHofste</td>\n",
       "      <td>hybas</td>\n",
       "      <td>PFAF_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v1c</td>\n",
       "      <td>tif</td>\n",
       "      <td>hybas_lev06_v1c_merged_fiona_30s_V01</td>\n",
       "      <td>merged</td>\n",
       "      <td>hybas</td>\n",
       "      <td>fiona</td>\n",
       "      <td>30s</td>\n",
       "      <td>V01</td>\n",
       "      <td>-9999</td>\n",
       "      <td>RutgerHofste</td>\n",
       "      <td>hybas</td>\n",
       "      <td>PFAF_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v1c</td>\n",
       "      <td>tif</td>\n",
       "      <td>hybas_lev06_v1c_merged_fiona_5min_V01</td>\n",
       "      <td>merged</td>\n",
       "      <td>hybas</td>\n",
       "      <td>fiona</td>\n",
       "      <td>5min</td>\n",
       "      <td>V01</td>\n",
       "      <td>-9999</td>\n",
       "      <td>RutgerHofste</td>\n",
       "      <td>hybas</td>\n",
       "      <td>PFAF_ID</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WWFversion extension                               fileName  \\\n",
       "1        v1c       tif   hybas_lev00_v1c_merged_fiona_30s_V01   \n",
       "2        v1c       tif  hybas_lev00_v1c_merged_fiona_5min_V01   \n",
       "3        v1c       tif   hybas_lev06_v1c_merged_fiona_30s_V01   \n",
       "4        v1c       tif  hybas_lev06_v1c_merged_fiona_5min_V01   \n",
       "\n",
       "  geographic_range indicator library spatial_resolution version  nodata  \\\n",
       "1           merged     hybas   fiona                30s     V01   -9999   \n",
       "2           merged     hybas   fiona               5min     V01   -9999   \n",
       "3           merged     hybas   fiona                30s     V01   -9999   \n",
       "4           merged     hybas   fiona               5min     V01   -9999   \n",
       "\n",
       "    ingested_by exportdescription    units  \n",
       "1  RutgerHofste             hybas  PFAF_ID  \n",
       "2  RutgerHofste             hybas  PFAF_ID  \n",
       "3  RutgerHofste             hybas  PFAF_ID  \n",
       "4  RutgerHofste             hybas  PFAF_ID  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 25.00 elapsed:  0:00:00.003318\n",
      "2 50.00 elapsed:  0:00:01.193377\n",
      "3 75.00 elapsed:  0:00:02.446587\n",
      "4 100.00 elapsed:  0:00:03.685470\n"
     ]
    }
   ],
   "source": [
    "df_errors = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "for index, row in df.iterrows():\n",
    "    elapsed_time = time.time() - start_time \n",
    "    print(index,\"%.2f\" %((index/4)*100), \"elapsed: \", str(timedelta(seconds=elapsed_time)))\n",
    "    df_errors2 = uploadEE(index,row)\n",
    "    df_errors = df_errors.append(df_errors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>command</th>\n",
       "      <th>error</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/anaconda3/bin/earthengine upload image --...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Started upload task with ID: EL4VETOITP4KFTJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/anaconda3/bin/earthengine upload image --...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Started upload task with ID: 6ULGGDDUZFMYSVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/anaconda3/bin/earthengine upload image --...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Started upload task with ID: 576HF6GYYSNFQSJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/anaconda3/bin/earthengine upload image --...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Started upload task with ID: 4HGTKRZJ4V7SJ7L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             command  error  \\\n",
       "1  /opt/anaconda3/bin/earthengine upload image --...      0   \n",
       "2  /opt/anaconda3/bin/earthengine upload image --...      0   \n",
       "3  /opt/anaconda3/bin/earthengine upload image --...      0   \n",
       "4  /opt/anaconda3/bin/earthengine upload image --...      0   \n",
       "\n",
       "                                            response  \n",
       "1  b'Started upload task with ID: EL4VETOITP4KFTJ...  \n",
       "2  b'Started upload task with ID: 6ULGGDDUZFMYSVI...  \n",
       "3  b'Started upload task with ID: 576HF6GYYSNFQSJ...  \n",
       "4  b'Started upload task with ID: 4HGTKRZJ4V7SJ7L...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 35",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
